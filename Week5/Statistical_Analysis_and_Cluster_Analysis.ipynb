{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb6ac17",
   "metadata": {},
   "source": [
    "# Statistical Analysis and Cluster Analysis\n",
    "\n",
    "## Overview\n",
    "In this lecture, we will learn how to cluster geographical regions based on their attributes. We will start from simple statistics, which are correlations and regression. Then, we move on to clustering analysis and investigate the two well-known methods, K-means clustering, and hierarchical clustering. \n",
    "\n",
    "In python, there are numerous packages that support statistics and cluster analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421d93b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60a910d",
   "metadata": {},
   "source": [
    "## Correlation and Regression: Boston housing price\n",
    "\n",
    "Focusing on Boston, we will investigate how the variables, such as crime rate, number of rooms, etc, are correlated with the housing price. Then, examine how well their variations explain the variations in housing price, with Ordinary Least Squares (OLS) regression. \n",
    "\n",
    "The followings are the columns in the `boston` DataFrame. The data is obtained from http://lib.stat.cmu.edu/datasets/boston.\n",
    "```python\n",
    "#  CRIM     per capita crime rate by town\n",
    "#  ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "#  INDUS    proportion of non-retail business acres per town\n",
    "#  CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "#  NOX      nitric oxides concentration (parts per 10 million)\n",
    "#  RM       average number of rooms per dwelling\n",
    "#  AGE      proportion of owner-occupied units built prior to 1940\n",
    "#  DIS      weighted distances to five Boston employment centres\n",
    "#  RAD      index of accessibility to radial highways\n",
    "#  TAX      full-value property-tax rate per \\$10,000\n",
    "#  PTRATIO  pupil-teacher ratio by town\n",
    "#  LSTAT    % lower status of the population\n",
    "#  MEDV     Median value of owner-occupied homes in $1000's\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260ebf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "data = pd.DataFrame(data, columns=['CRIM', 'ZN', 'INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO', 'B', 'LSTAT'])\n",
    "target = raw_df.values[1::2, 2]\n",
    "boston = data.merge(pd.DataFrame(target, columns=['MEDV']), left_index=True, right_index=True)\n",
    "boston.drop(columns=['B'], inplace=True)\n",
    "\n",
    "boston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66a8d75",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "Correlation means a statistical relationship between two variables, regardless of causality. The range of output is between -1 (negative) and +1 (positive) with 0 implying no correlation. There are several methods that reveal the correlation between variables, but in our lecture, we will investigate two major ones. \n",
    "* **Pearson's correlation** measures the linear relationship between two datasets. The calculation of the p-value relies on the assumption that each dataset is normally distributed. \n",
    "* **Spearman's correlation** does not assume that both datasets are normally distributed, unlike Pearson's correlation. It sorts the distribution of each variable, respectively, then compares their correlation. \n",
    "\n",
    "For the implementation in Python, we will utilize `scipy` package. The implementation of both Pearson and Spearman correlation is straightforward. They just require two inputs; distributions of two variables. For more information, visit <a href=https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html>scipy.stats.pearsonr</a> or <a href=https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html>scipy.stats.spearmanr</a>. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cbc437",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr_result = pearsonr(boston['CRIM'], boston['MEDV'])\n",
    "print(pearsonr_result)\n",
    "print(f\"Pearson's r between crime rate and home median value has r {round(pearsonr_result[0],2)} and p-value {round(pearsonr_result[1],2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be0784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr_result = spearmanr(boston['CRIM'], boston['MEDV'])\n",
    "print(spearmanr_result)\n",
    "print(f\"Spearman correlation between crime rate and home median value has r {round(spearmanr_result[0],2)} and p-value {round(spearmanr_result[1],2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0afc764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "independent_variables = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT']\n",
    "dependent_variable = ['MEDV']\n",
    "\n",
    "for var in independent_variables:\n",
    "    result = pearsonr(boston[var], boston['MEDV'])\n",
    "    print(f'Correlation between {var} and home median value: r {round(result[0], 2)}, p-val {round(result[1], 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd1fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(boston[['CRIM', 'ZN', 'INDUS', 'RM', 'PTRATIO', 'MEDV']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d036bebc",
   "metadata": {},
   "source": [
    "### Regression: Ordinary least squares (OLS)\n",
    "\n",
    "Unlike correlation, regression requires a casualty between two variables, meaning that one variable has an impact on the other variable. Therefore, we name variables as **independent variables** and **dependent variables**. Here, we assumes that `'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT'` variables may have impacted on the housing price in Boston `MEDV`. \n",
    "\n",
    "For the implementation in Python, we utilize `statsmodels` package. Here, you need to be careful which variable has an impact on which variable. For more information, visit <a hef=https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html> statsmodels.regression.linear_model.OLS</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables\n",
    "ind = boston[['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT']]\n",
    "\n",
    "# Dependent variables (Median value of owner-occupied homes in $1000's)\n",
    "dep = boston[['MEDV']]\n",
    "\n",
    "ind = sm.add_constant(ind)\n",
    "model = sm.OLS(dep, ind)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9070409",
   "metadata": {},
   "source": [
    "The resulted summary provides extensive information, but the major thing we need to focus are `R-squared`, `Adj. R-squared` and `Prob (F-statistic)`. \n",
    "\n",
    "Although the entire model was significant, this implementation has some variables (`INDUS`, `AGE`) provided statistically insignificant results, so that we need to run the model one more time with two variables removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables\n",
    "## Insignificant varaibles (INDUS, AGE) removed\n",
    "ind = boston[['CRIM', 'ZN', 'CHAS', 'NOX', 'RM', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT']]\n",
    "\n",
    "# Dependent variables (Median value of owner-occupied homes in $1000's)\n",
    "dep = boston[['MEDV']]\n",
    "\n",
    "ind = sm.add_constant(ind)\n",
    "model = sm.OLS(dep, ind)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792bba7a",
   "metadata": {},
   "source": [
    "Once the OLS model is constructed, we can predict housing prices in Boston (`MEDV`) based on some arbitrary inputs. The order of the input still needs to match with the original input data. For more information, please visit <a href=https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.predict.html>`statsmodels.regression.linear_model.OLS.predict`</a>.\n",
    "\n",
    "The original order in the current model is `'CRIM', 'ZN', 'CHAS', 'NOX', 'RM', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b5a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbitrary_nums = [1,  # Constant\n",
    "                  0.25651,  # CRIM\n",
    "                  0,        # ZN\n",
    "                  0,        # CHAS\n",
    "                  0.538,    # NOX\n",
    "                  6.2085,   # RM\n",
    "                  3.2074,   # DIS\n",
    "                  5.0,      # RAD\n",
    "                  330.0,    # TAX\n",
    "                  19.05,    # PTRATIO\n",
    "                  11.36,    # LSTAT\n",
    "                 ]\n",
    "\n",
    "results.predict(arbitrary_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e47a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740fb8e3",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "For the clustering, we will utilize the Social Vulnerability Index data of Chicago. Again, the data is made up of four different categories of vulnerability index (Socioeconomic Status, Household Composition & Disability, Minority Status & Language, and Housing Type & Transportation). With the SVI dataset, we will implement **K-means** clustering and **hierarchical** clustering and investigate how the clustering results vary based on different criteria. \n",
    "\n",
    "<img src=\"./data/svi.jpg\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "### K-Means Clustering\n",
    "K-means clustering places the observations into a single or multi-dimensional real vector and allocates the observations into a predefined *K* number of groups based on the distance between observations. The aim of this method is to minimize the variation within clusters. Therefore, quality is indicated by the number of clusters (i.e. K) as a direct correlation. \n",
    "\n",
    "$$ \\huge \\underset{\\text{S}}{\\text{arg min}} \\sum_{k}^{i=1}\\sum_{x\\in S_i}^{}\\left\\| x - \\mu_i \\right\\|^2$$\n",
    "\n",
    "For more information, please visit <a href=https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html>sklearn.cluster.KMeans</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8fbb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "svi = gpd.read_file('./data/svi_chicago.shp')\n",
    "svi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d27a2b",
   "metadata": {},
   "source": [
    "Let's place the variations into a two-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fd4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "svi1 = svi['RPL_THEME1']\n",
    "svi2 = svi['RPL_THEME2']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "ax.scatter(svi1, svi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24781c0",
   "metadata": {},
   "source": [
    "Conducting K-means clustering is very straightforward. You just need to specify the number of clusters in the `KMeans()` function, and then feed the function with the array you would like to cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(svi[['RPL_THEME1', 'RPL_THEME2']])\n",
    "\n",
    "# `.labels_` attribute provides which cluster each observation is assigned to\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95e1323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c','#fdbf6f','#ff7f00','#cab2d6']\n",
    "\n",
    "svi['label'] = kmeans.labels_\n",
    "svi['color'] = ''\n",
    "\n",
    "# Assign colors based on the clustering result\n",
    "for idx, row in svi.iterrows():\n",
    "    if row['label'] == 0:\n",
    "        svi.at[idx, 'color'] = colors[0]\n",
    "    elif row['label'] == 1:\n",
    "        svi.at[idx, 'color'] = colors[1]\n",
    "    elif row['label'] == 2:\n",
    "        svi.at[idx, 'color'] = colors[2]\n",
    "    elif row['label'] == 3:\n",
    "        svi.at[idx, 'color'] = colors[3]\n",
    "    elif row['label'] == 4:\n",
    "        svi.at[idx, 'color'] = colors[4]\n",
    "        \n",
    "# or \n",
    "svi['color'] = svi.apply(lambda x:colors[x['label']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a1b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `.cluster_centers_` provides the center of each cluster\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0ca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_.transpose()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d0d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Observations that are colored based on their clustering result\n",
    "ax.scatter(svi1, svi2, color=svi['color'])\n",
    "\n",
    "# Center of each cluster\n",
    "ax.scatter(kmeans.cluster_centers_.transpose()[0], \n",
    "           kmeans.cluster_centers_.transpose()[1], \n",
    "           color='black', s=100\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "svi.plot(color=svi['color'], figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7747870",
   "metadata": {},
   "source": [
    "How can we implement K-means clustering with 3D array? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfc7e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf7387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(svi[['RPL_THEME1', 'RPL_THEME2', 'RPL_THEME3']])\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc43c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "svi['label'] = kmeans.labels_\n",
    "svi['color'] = svi.apply(lambda x:colors[x['label']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f92acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook  \n",
    "fig = plt.figure( figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "# Observations that are colored based on their clustering result\n",
    "ax.scatter3D(svi['RPL_THEME1'], svi['RPL_THEME2'], svi['RPL_THEME3'], color=svi['color'])\n",
    "\n",
    "# Center of each cluster\n",
    "ax.scatter3D(kmeans.cluster_centers_.transpose()[0], \n",
    "             kmeans.cluster_centers_.transpose()[1], \n",
    "             kmeans.cluster_centers_.transpose()[2], \n",
    "             color='black',\n",
    "             s=50\n",
    "            )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f1cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "svi.plot(svi['color'], figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac212496",
   "metadata": {},
   "source": [
    "What if we consider every variable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(svi[['RPL_THEME1', 'RPL_THEME2', 'RPL_THEME3', 'RPL_THEME4']])\n",
    "\n",
    "svi['label'] = kmeans.labels_\n",
    "svi['color'] = svi.apply(lambda x:colors[x['label']], axis=1)\n",
    "\n",
    "svi.plot(svi['color'], figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacee97a",
   "metadata": {},
   "source": [
    "Or, it is also possible to conduct K-means for 1-D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a189035",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1718ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(svi[['RPL_THEMES']])\n",
    "\n",
    "svi['label'] = kmeans.labels_\n",
    "svi['color'] = svi.apply(lambda x:colors[x['label']], axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Observations that are colored based on their clustering result\n",
    "ax[0].scatter(svi[['RPL_THEMES']], [0 for i in range(len(svi['RPL_THEMES']))], color=svi['color'])\n",
    "\n",
    "# Center of each cluster\n",
    "ax[0].scatter(kmeans.cluster_centers_.transpose()[0], \n",
    "              [0 for i in range(len(kmeans.cluster_centers_.transpose()[0]))],\n",
    "              color='black', s=100\n",
    "             )\n",
    "\n",
    "svi.plot('label', ax=ax[1], color=svi['color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56272ec",
   "metadata": {},
   "source": [
    "How can we determine the **optimal number of clusters**, given that the quality of K-Means clustering is indicated by the number of clusters (i.e. K) as a direct correlation?\n",
    "\n",
    "There is a method called, **Silhouette Method**, which evaluates the current partitioning of distribution based on the equation below. \n",
    "\n",
    "$$\\huge S = \\frac{1}{N}\\sum_{i}^{N}\\frac{b(i)- a(i)}{\\text{max}{\\left\\{ a(i), b(i) \\right\\}}}$$\n",
    "\n",
    "where<br>\n",
    "$S$: the average Silhouette coefficients of current partitioning. <br>\n",
    "$N$: the number of points. <br>\n",
    "$a(i)$: a cohesion indicator of a point i (i.e. distances from point i to all other points in the same cluster). <br>\n",
    "$b(i)$: a separation indicator of a point i (i.e. distances from point i to all points in the other clusters). <br>\n",
    "$d_0$: the threshold travel cost of the analysis. \n",
    "\n",
    "The Silhouette method considers both within-cluster variation and between-cluster variation, and a **higher average Silhouette coefficient (i.e. S)** means that the current clustering is well classified.\n",
    "\n",
    "For more information, please visit <a href=https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html>`sklearn.metrics.silhouette_score`</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41804c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def determine_number_of_cluster(array):\n",
    "    km_silhouette = []\n",
    "\n",
    "    for i in range(2, 11):\n",
    "        KM = KMeans(n_clusters=i, n_init=100, max_iter=999)\n",
    "        KM.fit(array)\n",
    "\n",
    "        # Calculate Silhouette Scores\n",
    "        preds = KM.predict(array)\n",
    "        silhouette = silhouette_score(array, preds)\n",
    "        km_silhouette.append(silhouette)\n",
    "\n",
    "    return km_silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61514d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_silhoutte = determine_number_of_cluster(svi[['RPL_THEME1', 'RPL_THEME2', 'RPL_THEME3', 'RPL_THEME4']])\n",
    "k_means_silhoutte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8391c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "plt.plot(range(2, 11), k_means_silhoutte, color='black', linewidth='3', marker='o', linestyle='solid')\n",
    "\n",
    "ax.set_ylabel(f'Silhouette \\n coefficient', rotation='vertical', fontsize=20)\n",
    "ax.set_xlabel('Number of clusters', fontsize=20)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97202ead",
   "metadata": {},
   "source": [
    "### Hierarchical clustering\n",
    "\n",
    "The hierarchical clustering method is initiated with every individual element (i.e. each hour in our application), aggregating observations into a higher cluster based on the distances between them (i.e. similarity). Different from the results of K-means clustering are subject to change based on K, hierarchical clustering can employ multiple linkage criteria (e.g. single, complete, or Wardâ€™s method) to pursue the optimal results depending on the situation. \n",
    "\n",
    "For more information, please visit <a href=https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html>`scipy.cluster.hierarchy.linkage`</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8d969f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdcdcb8",
   "metadata": {},
   "source": [
    "First linkage method to investigate is **single**, which is defined as below. \n",
    "\n",
    "$$d(u, v) = \\text{min}\\left ( dist\\left (u\\left [ i \\right ], v\\left [ j \\right ] \\right) \\right )$$\n",
    "\n",
    "where<br>\n",
    "$i$: all points in cluster $u$ <br>\n",
    "$j$: all points in cluster $v$ <br>\n",
    "\n",
    "This linkage criteria is also known as Nearest Point Algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e565bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = hierarchy.linkage(svi[['RPL_THEMES']], method='single')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "dn = hierarchy.dendrogram(Z=Z, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33163c0d",
   "metadata": {},
   "source": [
    "The next linkage method to investigate is **complete**, which is defined as below. \n",
    "\n",
    "$$d(u, v) = \\text{max}\\left ( dist\\left (u\\left [ i \\right ], v\\left [ j \\right ] \\right) \\right )$$\n",
    "\n",
    "where<br>\n",
    "$i$: all points in cluster $u$ <br>\n",
    "$j$: all points in cluster $v$ <br>\n",
    "\n",
    "This linkage criteria is also known as the Farthest Point Algorithm or Voor Hees Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e197bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = hierarchy.linkage(svi[['RPL_THEMES']], method='complete')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "dn = hierarchy.dendrogram(Z=Z, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a1a90",
   "metadata": {},
   "source": [
    "The last linkage criteria to investigate is **Ward's method**, in which the total within-cluster variance is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe57c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = hierarchy.linkage(svi[['RPL_THEMES']], method='ward')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "dn = hierarchy.dendrogram(Z=Z, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9793e0",
   "metadata": {},
   "source": [
    "In general, hierarchical clustering does not indicate the optimal number of clusters, and it rather shows how the clustering aggregates. To assign geographical locations to each cluster, you need to employ `sklearn.cluster.AgglomerativeClustering()` method. In this method, you will specify how many clusters you want to have. You can reference the resulted dendrogram to come up with the optimal number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac1339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_cluster = AgglomerativeClustering(n_clusters=3, linkage='ward').fit(svi[['RPL_THEMES']])\n",
    "\n",
    "svi['label'] = agg_cluster.labels_\n",
    "svi['color'] = svi.apply(lambda x:colors[x['label']], axis=1)\n",
    "\n",
    "svi.plot(svi['color'], figsize=(10,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
